This is an application of Binary Search.
The only difference is we need to identify the boundaries of our search.

STEP 1: Let start = 1 and end = 2, indicating current search positions
This step can be performed in O(1) time

STEP 2: If A[end] < x, multiply start and end by 2 then repeat STEP 2 (until the inequality is no longer satisfied).
For example, if start is 1 and end is 2 then the new values are 2 and 4, respectively.
This step can be performed in O(log(n)) time

STEP 3: If A[start] > x or A[end] < x, return 'NO'
This step can be performed in O(1) time

STEP 4: Let m = ⌊(start + end)/2)⌋
Where Lx⌋ is the mathematical symbol indicating floor of x and ⌈x⌉ indicates ceiling of x.
For example, if start is 1 and end 6, ⌊(start + end)/2)⌋ = ⌊3.5⌋ = 3
This step can be performed in O(1) time

STEP 5: If x > A[m] then start = m + 1 and we return to STEP 3
This step can be performed in O(1) time

STEP 6: Else If x < A[m] then end = m - 1 and we return to STEP 3
This step can be performed in O(1) time

STEP 7: Else -> we know x = A[m] and we return index m
This step can be performed in O(1) time

Correctness:
The challenge in this problem is the boundaries of our search, videlicet, the right boundary.
This is tackled in STEPS 1 and 2 by exponentially expanding our search.
This results in logarithmic time to identify the boundaries.
STEPS 3-7 are simply Binary Search. We can do a binary search because our array is already sorted.
One case to consider is what happens when we encounter infinity after index n?
This does not violate the inequalities in STEPS 2, 3, and 6 because infinity is strictly larger than
any finite number in A. This also hold if our array is empty (only has infinities) and no finite numbers.

Running Time:
STEPS 3-7 are binary search and have O(logn) time
STEP 2 is O(logn) because we terminate our search upon encountering infinity.
This is similar to traversing a binary tree in reverse (bottom -> top) if we start ~1 and terminate ~n
and the height of the tree is log(n).

According to Master's theorem, we have T[n] = T[n/2] + O(1) for the recursion in STEPS 3-7.
This is due to having 1 subproblem when we recurse having size n/2
a = 1, b = 2, d = 0 => logb(a) => T[n] = O(n^d * logn) = O(logn)
Again, in STEP 2 we are doing binary search in reverse to find n and we have O(log(n)) as well.

O(logn) + O(logn) = O(logn)

**********************

I studied solo. No study group :(
